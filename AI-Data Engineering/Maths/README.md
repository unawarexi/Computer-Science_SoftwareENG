# Math for AI - 3 Months Learning Plan

---

## Month 1: Linear Algebra & Calculus

---

### Week 1: Vectors and Matrices
- Introduction to vectors: Vector operations (addition, scalar multiplication, dot product).
- Introduction to matrices: Matrix operations (addition, multiplication, transpose).
- Identity matrix, inverse matrix, and matrix transformations.

**Resources**:
- Linear algebra crash course.
- Matrix operations in Python (using Numpy).

---

### Week 2: Eigenvalues, Eigenvectors, and Matrix Decompositions
- Understanding eigenvalues and eigenvectors.
- Principal Component Analysis (PCA) in AI.
- Singular Value Decomposition (SVD) and its role in dimensionality reduction.

**Tools**:
- Numpy, Scikit-learn for PCA and SVD.

**Resources**:
- Linear algebra tutorials on eigenvalues and eigenvectors.
- PCA and SVD applications in machine learning.

---

### Week 3: Derivatives and Differentiation
- Basics of calculus: Limits, derivatives, and rules of differentiation.
- Partial derivatives and gradients.
- Gradient Descent: How it works in machine learning models.

**Tools**:
- SymPy for symbolic calculus in Python.

**Resources**:
- Calculus for machine learning guide.
- Gradient Descent explained.

---

### Week 4: Multivariable Calculus
- Multivariable functions: Partial derivatives and gradient vectors.
- Jacobian and Hessian matrices.
- Chain rule, Taylor series expansion, and optimization in AI models.

**Resources**:
- Multivariable calculus tutorials.
- Applications of Jacobian and Hessians in machine learning.

---

## Month 2: Probability & Statistics for AI

---

### Week 5: Basics of Probability Theory
- Understanding probability: Random variables, probability distributions (discrete and continuous).
- Conditional probability and Bayes’ theorem.
- Joint and marginal distributions.

**Resources**:
- Probability theory tutorials.
- Practical applications of Bayes’ theorem in AI.

---

### Week 6: Common Probability Distributions
- Introduction to key probability distributions: Bernoulli, Binomial, Poisson, Normal (Gaussian), Exponential.
- Expectation, variance, and covariance.

**Tools**:
- Numpy, Scipy for generating and analyzing probability distributions.

**Resources**:
- Overview of probability distributions used in AI.

---

### Week 7: Statistical Inference & Hypothesis Testing
- Introduction to statistical inference: Population, samples, and estimators.
- Confidence intervals, hypothesis testing, and p-values.
- Introduction to t-tests, chi-squared tests, and ANOVA.

**Tools**:
- Scipy for statistical tests in Python.

**Resources**:
- Hypothesis testing and confidence intervals in data science.

---

### Week 8: Bayesian Inference & Probabilistic Models
- Understanding Bayesian inference: Prior, likelihood, posterior.
- Bayesian networks and their applications in AI.
- Introduction to Markov Chains and Hidden Markov Models (HMM).

**Resources**:
- Bayesian inference tutorials.
- Introduction to probabilistic graphical models.

---

## Month 3: Advanced Topics in Optimization & Information Theory

---

### Week 9: Optimization Techniques
- Understanding convex functions and convex optimization.
- Gradient-based optimization: Stochastic Gradient Descent (SGD), momentum, RMSprop, and Adam.
- Lagrange multipliers and constraint optimization.

**Tools**:
- TensorFlow/PyTorch for implementing gradient-based optimization.

**Resources**:
- Optimization techniques in deep learning.

---

### Week 10: Information Theory
- Introduction to information theory: Entropy, cross-entropy, and Kullback-Leibler (KL) divergence.
- Mutual information and its role in AI models.
- Applications of information theory in machine learning, e.g., in loss functions and feature selection.

**Resources**:
- Information theory articles.
- Cross-entropy and KL divergence explained.

---

### Week 11: Regularization Techniques in AI
- Overfitting and underfitting: L1 (Lasso) and L2 (Ridge) regularization.
- Early stopping, dropout, and batch normalization techniques in deep learning.
- Regularization as an optimization problem.

**Tools**:
- Scikit-learn for L1 and L2 regularization.
- TensorFlow/Keras for deep learning regularization techniques.

**Resources**:
- Regularization techniques for machine learning.

---

### Week 12: Advanced Optimization & Reinforcement Learning Math
- Non-convex optimization: Dealing with local minima, saddle points.
- Policy gradients and Q-learning in reinforcement learning.
- Value function approximation and Bellman equations.

**Tools**:
- TensorFlow/PyTorch for implementing reinforcement learning models.

**Resources**:
- Optimization techniques in reinforcement learning.

---
